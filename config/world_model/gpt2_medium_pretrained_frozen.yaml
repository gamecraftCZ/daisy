_target_: models.TransformerConfig
tokens_per_block: 17
max_blocks: 20
attention: 'causal'
num_layers: 24
num_heads: 16
embed_dim: 1024
embed_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1
pretrained_weights: "gpt2-medium"
frozen: true